### Consumer Config
- auto.offset.reset : What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted)
  Valid Values:	[latest, earliest, none]
- enable.auto.commit : If true the consumer’s offset will be periodically committed in the background.
  Valid Values:	[true, false]
- auto.commit.interval.ms : The frequency in milliseconds that the consumer offsets are auto-committed to Kafka if enable.auto.commit is set to true
- retry.backoff.ms : The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios
- fetch.max.wait.ms : The maximum amount of time the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy the requirement given by fetch.min.bytes
- request.timeout.ms : The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted
- max.partition.fetch.bytes : The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted by the broker is defined via message.max.bytes (broker config) or max.message.bytes (topic config). See fetch.max.bytes for limiting the consumer request size
- fetch.min.bytes : The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. The default setting of 1 byte means that fetch requests are answered as soon as that many byte(s) of data is available or the fetch request times out waiting for data to arrive. Setting this to a larger value will cause the server to wait for larger amounts of data to accumulate which can improve server throughput a bit at the cost of some additional latency
- session.timeout.ms : The timeout used to detect client failures when using Kafka’s group management facility. The client sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received by the broker before the expiration of this session timeout, then the broker will remove this client from the group and initiate a rebalance. Note that the value must be in the allowable range as configured in the broker configuration by group.min.session.timeout.ms and group.max.session.timeout.ms

### Topic Level Configs
- cleanup.policy : This config designates the retention policy to use on log segments. The “delete” policy (which is the default) will discard old segments when their retention time or size limit has been reached. The “compact” policy will enable log compaction, which retains the latest value for each key. It is also possible to specify both policies in a comma-separated list (e.g. “delete,compact”). In this case, old segments will be discarded per the retention time and size configuration, while retained segments will be compacted
- compression.type : Specify the final compression type for a given topic. This configuration accepts the standard compression codecs (‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’). It additionally accepts ‘uncompressed’ which is equivalent to no compression; and ‘producer’ which means retain the original compression codec set by the producer.
- confluent.key.schema.validation : True if schema validation at record key is enabled for this topic.
- confluent.key.subject.name.strategy : Determines how to construct the subject name under which the key schema is registered with the schema registry. By default, TopicNameStrategy is used
- confluent.value.schema.validation : True if schema validation at record value is enabled for this topic.
- confluent.value.subject.name.strategy : Determines how to construct the subject name under which the value schema is registered with the schema registry. By default, TopicNameStrategy is used
- file.delete.delay.ms : The time to wait before deleting a file from the filesystem
- index.interval.bytes : This setting controls how frequently Kafka adds an index entry to its offset index. The default setting ensures that we index a message roughly every 4096 bytes. More indexing allows reads to jump closer to the exact position in the log but makes the index larger. You probably don’t need to change this.
- max.compaction.lag.ms : The maximum time a message will remain ineligible for compaction in the log. Only applicable for logs that are being compacted.
- max.message.bytes : The largest record batch size allowed by Kafka (after compression if compression is enabled). If this is increased and there are consumers older than 0.10.2, the consumers’ fetch size must also be increased so that they can fetch record batches this large. In the latest message format version, records are always grouped into batches for efficiency. In previous message format versions, uncompressed records are not grouped into batches and this limit only applies to a single record in that case.
- min.compaction.lag.ms : The minimum time a message will remain uncompacted in the log. Only applicable for logs that are being compacted.
- min.insync.replicas : When a producer sets acks to “all” (or “-1”), this configuration specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful. If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or NotEnoughReplicasAfterAppend).When used together, min.insync.replicas and acks allow you to enforce greater durability guarantees. A typical scenario would be to create a topic with a replication factor of 3, set min.insync.replicas to 2, and produce with acks of “all”. This will ensure that the producer raises an exception if a majority of replicas do not receive a write.
- retention.bytes : This configuration controls the maximum size a partition (which consists of log segments) can grow to before we will discard old log segments to free up space if we are using the “delete” retention policy. By default there is no size limit only a time limit. Since this limit is enforced at the partition level, multiply it by the number of partitions to compute the topic retention in bytes.
- retention.ms : This configuration controls the maximum time we will retain a log before we will discard old log segments to free up space if we are using the “delete” retention policy. This represents an SLA on how soon consumers must read their data. If set to -1, no time limit is applied.
- delete.retention.ms : The amount of time to retain delete tombstone markers for log compacted topics. This setting also gives a bound on the time in which a consumer must complete a read if they begin from offset 0 to ensure that they get a valid snapshot of the final stage (otherwise delete tombstones may be collected before they complete their scan).
- segment.bytes : This configuration controls the segment file size for the log. Retention and cleaning is always done a file at a time so a larger segment size means fewer files but less granular control over retention
- segment.index.bytes : This configuration controls the size of the index that maps offsets to file positions. We preallocate this index file and shrink it only after log rolls. You generally should not need to change this setting
- segment.jitter.ms : The maximum random jitter subtracted from the scheduled segment roll time to avoid thundering herds of segment rolling
- segment.ms : This configuration controls the period of time after which Kafka will force the log to roll even if the segment file isn’t full to ensure that retention can delete or compact old data.
- unclean.leader.election.enable : Indicates whether to enable replicas not in the ISR set to be elected as leader as a last resort, even though doing so may result in data loss.
